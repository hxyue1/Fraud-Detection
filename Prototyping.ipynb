{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used to prototype various ideas to be used in the Kaggle IEEE CIS Fraud-Detection competition. It is unlikely to be very well organised or well annotated as I will play around with ideas as I get sparks of inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard score proxy\n",
    "\n",
    "The first thing we want to do is to have an internal validation protocol that serves as a good proxy for the leaderboard score. There's no point in training models and tuning them if the metric we use isn't consistent with the dyanmics of the leaderboard score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Train-Test split\n",
    "\n",
    "This is the first idea we will be investigating. We will use 80% of the data for training and cross-validating to choose hyperparameters and the remaining 20% to test our model fit. We wil use xgboost as our prototyping model.\n",
    "\n",
    "sklearn has the train_test_split function to help us. An important question to ask is whether or not we should be shuffling the data. This is strictly speaking time-series, so shuffling data may not be appropriate, but this is transactional data so any time dependencies are unlikely to be very strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train_transaction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to do some data pre-processing as well. Nothing fancy, transformation of categorical variables into dummies and random forward filling of NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to deal with NAs by shuffling and forward filling.\n",
    "\n",
    "def ffill(df):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    na_count = df.isna().sum().sum()\n",
    "    while na_count>0:\n",
    "        df = df.sample(frac=1)\n",
    "        df = df.fillna(method='ffill',limit=10)\n",
    "        na_count = df.isna().sum().sum()\n",
    "\n",
    "        df = df.sort_index()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7184858322143555\n"
     ]
    }
   ],
   "source": [
    "#Only using the first 10000 rows otherwise my 11 inch 2014 MacBook Air won't be able to handle it :/\n",
    "train_sub = train.iloc[:12000,:]\n",
    "fraud = train_sub['isFraud']\n",
    "train_sub = train_sub.drop('isFraud', axis=1)\n",
    "\n",
    "#Numerics\n",
    "numerics = train_sub.select_dtypes(exclude='object')\n",
    "numerics = ffill(numerics)\n",
    "\n",
    "#Converting \n",
    "categorical = train_sub.select_dtypes(include='object')\n",
    "dummies = pd.get_dummies(categorical)\n",
    "\n",
    "X = pd.concat([numerics, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:10000,:],fraud.iloc[:10000],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496994633833445 0.8498410652920962\n"
     ]
    }
   ],
   "source": [
    "#Setting up model evaluation\n",
    "model = xgb.XGBClassifier(\n",
    "    max_depth = 300,\n",
    "    learning_rate = 0.1,\n",
    "    objective = 'binary:logistic',    \n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric = 'auc')\n",
    "\n",
    "train_preds = model.predict_proba(X_train)\n",
    "test_preds = model.predict_proba(X_test)\n",
    "\n",
    "train_score = roc_auc_score(y_train, train_preds[:,1])\n",
    "test_score = roc_auc_score(y_test, test_preds[:,1])\n",
    "\n",
    "print(train_score,test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so using a train test split is not too bad, though as expected, the training score is slightly lower than the test score. sklearn's train_test_split shuffles the data for us, I'm not sure if that is the right way to go, I'll check with an unshuffled 'final' test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496994633833445 0.8498410652920962 0.7357435334065714\n"
     ]
    }
   ],
   "source": [
    "final_test_preds = model.predict_proba(X.iloc[10000:12000,:])\n",
    "final_test_score = roc_auc_score(fraud.iloc[10000:12000], final_test_preds[:,1])\n",
    "print(train_score, test_score, final_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's certainly interesting. The test score on the unshuffled holdout set is even worse! This may mean that shuffling our data biases our in sample metrics upwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522543059777102 0.8063564101005598 0.7520473384600019\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.iloc[:10000,:],fraud.iloc[:10000],test_size=0.2, shuffle=False)\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric = 'auc')\n",
    "\n",
    "train_preds = model.predict_proba(X_train)\n",
    "test_preds = model.predict_proba(X_test)\n",
    "final_test_preds = model.predict_proba(X.iloc[10000:12000,:])\n",
    "\n",
    "train_score = roc_auc_score(y_train, train_preds[:,1])\n",
    "test_score = roc_auc_score(y_test, test_preds[:,1])\n",
    "final_test_score = roc_auc_score(fraud.iloc[10000:12000], final_test_preds[:,1])\n",
    "\n",
    "print(train_score, test_score, final_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, doesn't seem to be the case here at all. Shuffling doesn't seem to have any effect on the variation in the scores across the train, test and final test set. Rather it is the distance in time between the training set and the test set which causes the loss in generality. Let's see if we can test this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.767240047454834\n"
     ]
    }
   ],
   "source": [
    "train_sub = train.iloc[:100000,:]\n",
    "fraud = train_sub['isFraud']\n",
    "train_sub = train_sub.drop('isFraud', axis=1)\n",
    "\n",
    "#Numerics\n",
    "numerics = train_sub.select_dtypes(exclude='object')\n",
    "numerics = ffill(numerics)\n",
    "\n",
    "#Converting \n",
    "categorical = train_sub.select_dtypes(include='object')\n",
    "dummies = pd.get_dummies(categorical)\n",
    "\n",
    "X = pd.concat([numerics, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:10000,:]\n",
    "y_train = fraud.iloc[:10000]\n",
    "\n",
    "model.fit(X_train, y_train, eval_metric = 'auc')\n",
    "\n",
    "scores = []\n",
    "for index in [1,2,3,4,5,6,7,8,9]:\n",
    "    start = (index - 1)*1000\n",
    "    end = index*1000\n",
    "    preds = model.predict_proba(X.iloc[start:end,:])\n",
    "    test = fraud.iloc[start:end]\n",
    "    score = roc_auc_score(test,preds[:,1])\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9639255499153977, 0.9534038856420786, 0.8998076487893187, 0.9695824795081966, 0.9179829410835572, 0.9711627515365833, 0.95447006791976, 0.9578252032520326, 0.9702886423273208] 0.9509387966638051\n"
     ]
    }
   ],
   "source": [
    "print(scores, np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it was a false alarm, there is going to be inherent variation in the test scores because of the randomness of the data. However, when you average it out, it's pretty close to the training scores we're getting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9946369  0.9971597  0.9963649  0.98149884 0.9874941  0.9829064\n",
      " 0.9992461  0.94470257 0.9970029  0.9927669  0.9967974  0.99951786\n",
      " 0.99021554 0.9981632  0.99385613 0.99472964 0.9983237  0.9994455\n",
      " 0.9995038  0.9698555  0.9986267  0.697731   0.9989849  0.9960668\n",
      " 0.98827666 0.99734145 0.99609506 0.9959356  0.9985062  0.2418536\n",
      " 0.99712455 0.9990179  0.9993223  0.99455065 0.99712455 0.99745774\n",
      " 0.9820326  0.9959274  0.99609506 0.95665723 0.6432842  0.99609506\n",
      " 0.9162802  0.9889068  0.99918956 0.9974291  0.98828125 0.99649346\n",
      " 0.99574125 0.9994194  0.9959356  0.94470257 0.96324563 0.99927735\n",
      " 0.9970825  0.98827666 0.72831094 0.9805087  0.9985814  0.99948657\n",
      " 0.9986864  0.99574125 0.9988583  0.8720083  0.99367344 0.990801\n",
      " 0.9920441  0.9980923  0.99591863 0.9966662  0.9927669  0.9978156\n",
      " 0.9944435  0.9874742  0.92351556 0.3865987  0.9993223  0.9792002\n",
      " 0.98067313 0.3865987  0.9985352  0.99786395 0.82444745 0.9896744\n",
      " 0.99803275 0.96324563 0.9920441  0.99803275 0.9871452  0.3865987\n",
      " 0.3865987  0.98827666 0.9917265  0.9878072  0.95665723 0.3865987\n",
      " 0.99803275 0.75254905 0.94470257 0.9989849  0.99935204 0.994731\n",
      " 0.98705965 0.99762857 0.99868155 0.9962938  0.9973629  0.984199\n",
      " 0.998593   0.99929285 0.9927669  0.9961447  0.99908155 0.998243\n",
      " 0.99957293 0.99812305 0.9931544  0.99192876 0.89319396 0.99892235\n",
      " 0.9970891  0.9971096  0.9917265  0.329803   0.99786395 0.9982922\n",
      " 0.9963449  0.9978943  0.996389   0.9960027  0.98672295 0.9993223\n",
      " 0.9970825  0.98189485 0.99803275 0.98308176 0.9564643  0.99950266\n",
      " 0.9680185  0.99842465 0.99929285 0.91296476 0.99842465 0.998593\n",
      " 0.9974964  0.9985814  0.99645805 0.9792002  0.99789155 0.9006264\n",
      " 0.99692917 0.9984937  0.9713326  0.997682   0.99416554 0.9972488\n",
      " 0.8832755  0.91296476 0.97145605 0.9994501  0.9982922  0.9591348\n",
      " 0.98672295 0.99894583 0.9927669  0.99803275 0.9982922  0.99598193\n",
      " 0.98672295 0.99662054 0.99334556 0.9984675  0.70993966 0.9504775\n",
      " 0.99334556 0.98886126 0.65211225 0.9593597  0.9992456  0.9950982\n",
      " 0.9962509  0.9995758  0.998593   0.9713326  0.96324563 0.9970029\n",
      " 0.98072976 0.99951786 0.99091315 0.9993372  0.99749136 0.9917489\n",
      " 0.996492   0.9891673  0.9976962  0.99318445 0.99908197 0.99803245\n",
      " 0.9959274  0.998854   0.9994124  0.99553645 0.99091315 0.9976649\n",
      " 0.9854003  0.9909589  0.99691886 0.9991548  0.997884   0.9102368\n",
      " 0.9982481  0.99782723 0.9989191  0.99447966 0.99544376 0.98827666\n",
      " 0.994205   0.99472964 0.97939414 0.9958509  0.9980923  0.98827666\n",
      " 0.99333334 0.99520606 0.99091315 0.9989849  0.99712455 0.9987811\n",
      " 0.9952339  0.99649346 0.93895173 0.9975963  0.99318445 0.9981646\n",
      " 0.99733275 0.99724156 0.99649346 0.9894681  0.99898416 0.996448\n",
      " 0.9778556  0.9996194  0.997884   0.99645907 0.98672295 0.99554807\n",
      " 0.9906846  0.9963262  0.9792002  0.997884   0.9972929  0.99367344\n",
      " 0.9971597  0.9927669  0.99091315 0.98068136 0.99598193 0.9976649\n",
      " 0.99877506 0.9602701  0.9974392  0.9984675  0.8590123  0.9917265\n",
      " 0.9990092  0.9969388  0.9962626  0.16802043 0.9984675  0.9961628\n",
      " 0.990426   0.9564643  0.99878055 0.9970029  0.990426   0.9881279\n",
      " 0.9917265  0.91296476 0.99679506 0.98068136 0.99649346 0.9965657\n",
      " 0.96324563 0.99091315 0.996389   0.99878055 0.9976861  0.9863531\n",
      " 0.9985062  0.9982481  0.9977327  0.98068136 0.8540395  0.17420942\n",
      " 0.9976861  0.98886126 0.99835485 0.9935041  0.9324188  0.99472964\n",
      " 0.9871452  0.9970029  0.98962635 0.99679494 0.9698555  0.9102368\n",
      " 0.99416554 0.9906876  0.9979507  0.9991748  0.99591863 0.11791176\n",
      " 0.99198747 0.997884   0.9950982  0.99724156 0.94470257 0.9950982\n",
      " 0.9874941  0.93682826 0.9950982  0.9934091  0.92351556 0.72854507\n",
      " 0.96324563 0.99759614 0.9982922  0.9897849  0.99781317 0.99579644\n",
      " 0.9887516  0.681229   0.9991384  0.99449694 0.9917265  0.9713326\n",
      " 0.99553    0.9981632  0.99686855 0.9959356  0.9981144  0.9950982\n",
      " 0.996389   0.9993173  0.99866337 0.9974187  0.9978156  0.99896884\n",
      " 0.9991384  0.9971748  0.99842745 0.9986267  0.9982922  0.99649346\n",
      " 0.9959274  0.9891673  0.9970825  0.9868187  0.9987107  0.96324563\n",
      " 0.9987146  0.99293584 0.99745774 0.99469393 0.9991821  0.94470257\n",
      " 0.9950982  0.99803275 0.9990848  0.16802043 0.99318445 0.9987903\n",
      " 0.9976408  0.9934091  0.9958703  0.9882995  0.9991661  0.99957293\n",
      " 0.9989408  0.9564643  0.99416554 0.99416554 0.95889807 0.99591863\n",
      " 0.99759614 0.99745774 0.95889807 0.42909616 0.99811375 0.99782723\n",
      " 0.60892    0.99416554 0.3865987  0.9989408  0.95889807 0.6801729\n",
      " 0.9995758  0.9932706  0.9917265  0.99416554 0.9925947  0.95889807\n",
      " 0.9984741  0.9983746  0.9979058  0.98827666 0.9982442  0.99881124\n",
      " 0.9963262  0.9987811  0.9989849  0.99943256 0.8720083  0.9984675\n",
      " 0.99804676 0.98380053 0.7305771  0.9980923  0.94990504 0.98708254\n",
      " 0.95665723 0.99829984 0.9980301  0.9927669  0.9966075  0.99803275\n",
      " 0.9989849  0.99732554 0.9730158  0.95889807 0.99076205 0.95889807\n",
      " 0.9981632  0.9713326  0.99091315 0.99679506 0.99591863 0.984199\n",
      " 0.998554   0.9993372  0.99334556 0.92351556 0.9970361  0.93895173\n",
      " 0.9871452  0.9974741  0.99155515 0.98068136 0.93895173 0.9960027\n",
      " 0.9932419  0.9971979  0.98886126 0.997952   0.9988647  0.9799606\n",
      " 0.99490887 0.97634304 0.9905373  0.9970825  0.9973232  0.98720926\n",
      " 0.9985062  0.97304785 0.9951815  0.98720926 0.99416554 0.9982481\n",
      " 0.99512553 0.98827666 0.9964612  0.9989849  0.99472964 0.9959356\n",
      " 0.99849224 0.9781823  0.996492   0.9958509  0.9832031  0.99849224\n",
      " 0.9827904  0.82444745 0.99712455 0.83026135 0.99091315 0.99842465\n",
      " 0.98827666 0.9986708  0.9927669  0.998417   0.98672295 0.99935204\n",
      " 0.93895173 0.98786145 0.9927566  0.99812305 0.986315   0.93895173\n",
      " 0.9992225  0.65211225 0.9874941  0.9891375  0.998854   0.9981646\n",
      " 0.9069061  0.9992398  0.9768343  0.99845815 0.998575   0.9871452\n",
      " 0.986842   0.99728864 0.997202   0.99447966 0.995879   0.99776787\n",
      " 0.93895173 0.99745774 0.9148243  0.99746454 0.98131114 0.9993372\n",
      " 0.9989408  0.995879   0.99869156 0.9974187  0.94470257 0.9960402\n",
      " 0.9959274  0.99228716 0.9148243  0.9832031  0.9148243  0.9979968\n",
      " 0.9996194  0.99768734 0.97989976 0.99548644 0.997884   0.95271486\n",
      " 0.991849   0.99662095 0.99803275 0.9938496  0.9996194  0.99548644\n",
      " 0.9996194  0.9963139  0.97544146 0.9974187  0.9927669  0.9504775\n",
      " 0.9970825  0.9996194  0.9971597  0.9959274  0.99957293 0.9972246\n",
      " 0.98010856 0.996389   0.98308176 0.98951226 0.97435534 0.9976649\n",
      " 0.9986968  0.7876954  0.9863594  0.99091315 0.9991661  0.963541\n",
      " 0.99849224 0.9961447  0.94990504 0.96324563 0.9982922  0.9987941\n",
      " 0.7876954  0.9980301  0.9985062  0.99965906 0.97554916 0.99416554\n",
      " 0.9879133  0.98951226 0.98720926 0.9976649  0.998153   0.9917265\n",
      " 0.99472964 0.98886126 0.9981632  0.9986267  0.99311996 0.9905373\n",
      " 0.9162802  0.9917265  0.99649346 0.9934091  0.9946365  0.9877817\n",
      " 0.9934091  0.96324563 0.3865987  0.9958509  0.9934718  0.998593\n",
      " 0.97519237 0.9960402  0.99888444 0.99662095 0.99647075 0.8540395\n",
      " 0.98720926 0.99728864 0.9984277  0.9995367  0.9977088  0.99645805\n",
      " 0.98698527 0.96324563 0.9935565  0.9854828  0.95148844 0.99781317\n",
      " 0.998554   0.9589515  0.98827666 0.96163076 0.99091315 0.6432842\n",
      " 0.99556327 0.9989849  0.99416554 0.99574125 0.9985352  0.97145605\n",
      " 0.9404644  0.99405676 0.8590123  0.99859935 0.9976848  0.9982922\n",
      " 0.99894583 0.99367344 0.98720926 0.9813733  0.9996125  0.996389\n",
      " 0.98380053 0.9971748  0.9985165  0.9932792  0.9983746  0.99091315\n",
      " 0.9976408  0.99091315 0.9939766  0.99623233 0.9981632  0.99269813\n",
      " 0.99950665 0.99740124 0.9934091  0.99091315 0.99416554 0.93975055\n",
      " 0.9960402  0.99686855 0.9971748  0.9991548  0.99679506 0.9983746\n",
      " 0.99645805 0.98827666 0.9872025  0.99647075 0.9698555  0.99762857\n",
      " 0.9971748  0.99762857 0.9987146  0.99028975 0.9963094  0.9982922\n",
      " 0.98827666 0.9974187  0.9989408  0.9984172  0.9980923  0.82444745\n",
      " 0.99091315 0.99363923 0.997884   0.9967974  0.98131114 0.9993223\n",
      " 0.4866197  0.9927669  0.9989408  0.65211225 0.99645907 0.99416554\n",
      " 0.9934091  0.9872025  0.9982922  0.50083554 0.9607324  0.3865987\n",
      " 0.99463844 0.99649346 0.99691886 0.3865987  0.9989408  0.9967774\n",
      " 0.96324563 0.65211225 0.99416554 0.99894494 0.99745774 0.9978362\n",
      " 0.99662054 0.9972329  0.97145605 0.9925947  0.9983674  0.9992456\n",
      " 0.99377185 0.96324563 0.99903697 0.9719879  0.9872025  0.98720926\n",
      " 0.9737546  0.9848535  0.98720926 0.99649346 0.9927669  0.99822414\n",
      " 0.9270008  0.9990708  0.8928096  0.9987826  0.9872025  0.98720926\n",
      " 0.9993223  0.9951199  0.99796176 0.9872025  0.9974964  0.99553\n",
      " 0.98672295 0.98720926 0.9994194  0.9953939  0.9954332  0.99803275\n",
      " 0.99405676 0.9935945  0.99649346 0.99630284 0.65211225 0.98672295\n",
      " 0.9965841  0.995331   0.9920441  0.99803275 0.99958205 0.99728864\n",
      " 0.9970029  0.4437722  0.99645805 0.9994596  0.9981835  0.997944\n",
      " 0.99803275 0.83679247 0.9706153  0.3865987  0.9148243  0.99367344\n",
      " 0.9985814  0.5754385  0.9983746  0.83679247 0.8720083  0.9989408\n",
      " 0.98068136 0.98672295 0.9981646  0.9991384  0.997202   0.9854003\n",
      " 0.3865987  0.61032456 0.9927669  0.98827666 0.9102368  0.9965841\n",
      " 0.93811464 0.9910019  0.9981417  0.8540395  0.9719879  0.9996194\n",
      " 0.9925187  0.99892    0.9917265  0.9987126  0.98380053 0.9979058\n",
      " 0.9380041  0.9996194  0.9970825  0.9713326  0.98827666 0.8911538\n",
      " 0.9935945  0.9959274  0.99728864 0.998469   0.9927669  0.9785025\n",
      " 0.99091315 0.9853981  0.9959356  0.98886126 0.9993223  0.9970029\n",
      " 0.93682826 0.997884   0.99405676 0.9959274  0.9872025  0.99881124\n",
      " 0.9573292  0.99645805 0.99579644 0.9980301  0.99929285 0.9994983\n",
      " 0.9602701  0.9102368  0.9713326  0.99091315 0.9917265  0.93895173\n",
      " 0.99881124 0.9994194  0.99645805 0.98827666 0.9960402  0.98068136\n",
      " 0.984199   0.9989172  0.99575496 0.9970029  0.9994961  0.993374\n",
      " 0.9996125  0.9934091  0.9719879  0.9917265  0.93895173 0.9055995\n",
      " 0.9991661  0.9934091  0.9927669  0.99830145 0.99728864 0.970875\n",
      " 0.984199   0.9804445  0.9936663  0.98951226 0.93682826 0.99649346\n",
      " 0.999431   0.99785644 0.91296476 0.99845815 0.9934091  0.9959356\n",
      " 0.9981835  0.98827666 0.9959356  0.99803275 0.99591863 0.9584691\n",
      " 0.9988523  0.99416554 0.9989172  0.9967974  0.99367344 0.9887516\n",
      " 0.96324563 0.9981646  0.95665723 0.9970825  0.99679506 0.96163076\n",
      " 0.99877506 0.9863157  0.9982922  0.9917265  0.99870276 0.9979058\n",
      " 0.9993223  0.99645805 0.9870699  0.9986267  0.9950982  0.9730213\n",
      " 0.99647075 0.9594432  0.9979264  0.99091315 0.9989408  0.9989849\n",
      " 0.99416554 0.99624443 0.9981646  0.9964086  0.9713326  0.92351556\n",
      " 0.99728864 0.98827666 0.96324563 0.9975822  0.9865582  0.999134\n",
      " 0.99728864 0.98308176 0.9940393  0.9980301  0.99714065 0.98672295\n",
      " 0.93895173 0.99774504 0.96005446 0.984199   0.99649346 0.9974187\n",
      " 0.9959629  0.99745774 0.98886126 0.99647075 0.9846792  0.9978588\n",
      " 0.99743575 0.9970825  0.99951786 0.99228716 0.9934718  0.9944285\n",
      " 0.99416554 0.9991661  0.9993372  0.99111354 0.8928096  0.85446167\n",
      " 0.9959274  0.9934091  0.93895173 0.9991661  0.9982922  0.9982711\n",
      " 0.99679506 0.98886126 0.98827666 0.9984652  0.75913584 0.98886126\n",
      " 0.9982922  0.9974187  0.9886366  0.93895173 0.75913584 0.8928096\n",
      " 0.9991384  0.75913584 0.75913584 0.99645805 0.9993372  0.99893004\n",
      " 0.9950982  0.9987456  0.9976796  0.9942223  0.99647075 0.93895173\n",
      " 0.9148243  0.998854   0.99405676 0.9991661  0.9713326  0.96324563\n",
      " 0.99845815 0.9769385  0.99835485 0.99862856]\n"
     ]
    }
   ],
   "source": [
    "    preds = model.predict_proba(X.iloc[start:end,:])\n",
    "    print(preds[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross-validation\n",
    "\n",
    "The next validation protocol we'll consider using is k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
