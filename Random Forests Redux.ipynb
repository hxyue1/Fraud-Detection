{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#train_identity = pd.read_csv('Data/train_identity.csv')\n",
    "train_transaction = pd.read_csv('Data/train_transaction.csv')\n",
    "#test_identity = pd.read_csv('Data/test_identity.csv')\n",
    "test_transaction = pd.read_csv('Data/test_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trans_rows = train_transaction.shape[0]\n",
    "test_trans_rows = test_transaction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One mistake I made in the previous notebook was not combining the test and train matrices together and applying the operations simultaneously. It lead to some dummies appearing in the training set, but not the test set and vice versa. We'll try and correct it here. We'll also create a separate dummy variable for categorical variables with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 392)\n",
      "(506691, 392)\n"
     ]
    }
   ],
   "source": [
    "fraud = train_transaction['isFraud']\n",
    "train_x_trans = train_transaction.drop(['isFraud','TransactionID'], axis=1)\n",
    "test_x_trans = test_transaction.drop('TransactionID', axis=1)\n",
    "X_trans = pd.concat([train_x_trans,test_x_trans])\n",
    "del train_transaction\n",
    "del test_transaction\n",
    "print(train_x_trans.shape)\n",
    "print(test_x_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract the numerics and strings and convert the strings to dummies. We'll convert the NaNs to dummies as well.  I don't think multicollinearity will be an issue with random forests. Any remaining NaNs, we'll fill with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = X_trans.select_dtypes(exclude='object')\n",
    "strings = X_trans.select_dtypes(include='object')\n",
    "dummies = pd.get_dummies(strings,dummy_na=True)\n",
    "X_trans = pd.concat([numerics, dummies], sort=False, axis=1)\n",
    "X_trans = X_trans.fillna(X_trans.mean())\n",
    "del numerics, strings, dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll separate back into training and test sets and start optimising our random forest by tuning the hyperparameters. Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590540, 544)\n",
      "(506691, 544)\n"
     ]
    }
   ],
   "source": [
    "train_x_trans = X_trans.iloc[:train_trans_rows,:]\n",
    "test_x_trans = X_trans.iloc[train_trans_rows:,:]\n",
    "del X_trans\n",
    "#Number of rows should be the same\n",
    "print(train_x_trans.shape)\n",
    "print(test_x_trans.shape)\n",
    "data = pd.concat([fraud, train_x_trans], axis=1)\n",
    "del train_x_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import model_selection as ms\n",
    "import noisyopt as no \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our black box Random Forests function\n",
    "#def RF_score(x):\n",
    "    \n",
    "    ##Naming hyperparemter inputs\n",
    "    #n_estimators = x[0]\n",
    "    #max_depth = x[1]\n",
    "    #min_samples_split = x[2]\n",
    "    #min_samples_leaf = x[3]\n",
    "    #max_features = x[4]\n",
    "\n",
    "def RF_score(n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features):\n",
    "    \n",
    "    #Contraining hyperparameters to be converted to integers (e.g. number of decision trees can't be continuous!)\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "    min_samples_split = int(min_samples_split)\n",
    "    min_samples_leaf = int(min_samples_leaf)\n",
    "    max_features = int(max_features)\n",
    "    \n",
    "    assert type(n_estimators) == int\n",
    "    assert type(max_depth) == int\n",
    "    assert type(min_samples_split) == int\n",
    "    assert type(min_samples_leaf) == int\n",
    "    assert type(max_features) == int\n",
    "    \n",
    "    param = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'max_features': max_features\n",
    "            }\n",
    "    \n",
    "    #Setting up Random Forest with input parameters\n",
    "    fraud_RFC = RFC(\n",
    "                    n_estimators=n_estimators, \n",
    "                    max_depth=max_depth, \n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf = min_samples_leaf,\n",
    "                    max_features = max_features)\n",
    "    \n",
    "    #Subsetting data    \n",
    "    data_sub = data\n",
    "    data_sub['RNG'] = np.random.random_sample(data_sub.shape[0])\n",
    "    data_sub = data_sub[data_sub['RNG'] <=0.01]\n",
    "    X = data_sub.iloc[:,1:]\n",
    "    y = data_sub.iloc[:,0]\n",
    "    \n",
    "    #Evaluating configuration using time series split\n",
    "    tscv = ms.TimeSeriesSplit(n_splits=3)\n",
    "    score = []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        fraud_RFC_fit = fraud_RFC.fit(X=X_train, y=y_train)\n",
    "        pred_probs = fraud_RFC_fit.predict_proba(X=X_test)\n",
    "        score.append(roc_auc_score(y_test, pred_probs[:,1]))\n",
    "\n",
    "\n",
    "    #cv_preds = ms.cross_val_predict(fraud_RFC, X=data_sub.iloc[:,1:], y=data_sub.iloc[:,0], cv=3,method='predict_proba')\n",
    "    #cv_score = roc_auc_score(data_sub.iloc[:,0],cv_preds[:,1])\n",
    "    return(np.mean(score))\n",
    "    \n",
    "    \n",
    "\n",
    "#Setting the boundaries for the hyperparemters to be tuned\n",
    "bounds_RF = {\n",
    "    'n_estimators': (10,3000),\n",
    "    'max_depth': (1,100),\n",
    "    'min_samples_split': (2,200),\n",
    "    'min_samples_leaf': (1,200),\n",
    "    'max_features': (1,544)\n",
    "}\n",
    "\n",
    "bounds = [\n",
    "    [10,3000],\n",
    "    [1,100],\n",
    "    [2,200],\n",
    "    [1,200],\n",
    "    [1,544]    \n",
    "]\n",
    "\n",
    "\n",
    "RF_BO = BayesianOptimization(RF_score, bounds_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | min_sa... | min_sa... | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8575  \u001b[0m | \u001b[0m 92.26   \u001b[0m | \u001b[0m 87.93   \u001b[0m | \u001b[0m 53.84   \u001b[0m | \u001b[0m 35.29   \u001b[0m | \u001b[0m 2.44e+03\u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8362  \u001b[0m | \u001b[0m 27.3    \u001b[0m | \u001b[0m 497.8   \u001b[0m | \u001b[0m 41.61   \u001b[0m | \u001b[0m 93.92   \u001b[0m | \u001b[0m 1.677e+0\u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "RF_BO.maximize(n_iter=100,alpha=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian optimisation seems to do a reasonable job at finding good hyperparameter combinations. It's clear that having a large number of estimators helps the model to generalise well, the relationship with the other hyperparameters is not so clear. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
