{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_identity = pd.read_csv('Data/train_identity.csv')\n",
    "train_transaction = pd.read_csv('Data/train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing variable names\n",
    "identity_vars = list(train_identity)\n",
    "transaction_vars = list(train_transaction)\n",
    "\n",
    "#Storing id, fraud and separating from explanatory variables\n",
    "trans_id = train_transaction['TransactionID']\n",
    "fraud = train_transaction['isFraud']\n",
    "x_trans = train_transaction.drop(['TransactionID','isFraud'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting dummies from strings\n",
    "strings = train_transaction.select_dtypes(include='object')\n",
    "numerics = train_transaction.select_dtypes(exclude='object')\n",
    "dummies = pd.get_dummies(strings)\n",
    "x_trans = pd.concat([dummies, numerics],axis=1)\n",
    "x_trans = x_trans.drop(['TransactionID','isFraud'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a count of NaNs\n",
    "transaction_na_count = x_trans.isnull().sum()\n",
    "transaction_na_prop = x_trans.isnull().sum()/x_trans.shape[0]*100\n",
    "transaction_na = pd.concat([transaction_na_count, transaction_na_prop], axis=1)\n",
    "transaction_na.columns = ['Count','Percentage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V257           0.262946\n",
      "V246           0.251838\n",
      "V244           0.249951\n",
      "V242           0.247522\n",
      "V45            0.236688\n",
      "V201           0.234520\n",
      "V200           0.227926\n",
      "V86            0.224530\n",
      "V87            0.224450\n",
      "V189           0.220374\n",
      "V44            0.218669\n",
      "V188           0.217058\n",
      "V258           0.203975\n",
      "V52            0.201111\n",
      "V51            0.187440\n",
      "V228           0.184556\n",
      "V170           0.178601\n",
      "V40            0.178413\n",
      "V79            0.173097\n",
      "V39            0.170565\n",
      "V94            0.167984\n",
      "V38            0.167128\n",
      "V43            0.166514\n",
      "V33            0.165534\n",
      "V199           0.164959\n",
      "V17            0.164800\n",
      "V18            0.164689\n",
      "V74            0.164684\n",
      "V34            0.162660\n",
      "V81            0.162608\n",
      "                 ...   \n",
      "M8_F           0.043108\n",
      "V75            0.046516\n",
      "D5             0.046812\n",
      "V12            0.047279\n",
      "M6_F           0.048760\n",
      "D2             0.051839\n",
      "D4             0.056450\n",
      "V36            0.058682\n",
      "D7             0.063238\n",
      "M7_F           0.063570\n",
      "V35            0.064763\n",
      "D10            0.064959\n",
      "D1             0.067118\n",
      "M6_T           0.068134\n",
      "M9_T           0.068434\n",
      "D15            0.069117\n",
      "M3_T           0.084136\n",
      "D8             0.084568\n",
      "M2_T           0.089079\n",
      "V70            0.089377\n",
      "M1_T           0.089400\n",
      "V69            0.092584\n",
      "V30            0.093002\n",
      "V49            0.093153\n",
      "V91            0.094022\n",
      "V48            0.097239\n",
      "V29            0.097299\n",
      "V90            0.098226\n",
      "card6_debit    0.099779\n",
      "ProductCD_W    0.135549\n",
      "Length: 529, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Filling nas with mean\n",
    "x_trans = x_trans.fillna(x_trans.mean())\n",
    "corrs = x_trans.corrwith(fraud)\n",
    "corrs = abs(corrs.sort_values(ascending=False))\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just copying stuff from the EDA notebook, nothing new. We'll first train a logistic regression on the entire dataset and get a ballpark figure for the AUC score, then we'll:\n",
    "1. Properly set up procedures for cross-validation and calculation of the AUC score\n",
    "2. Train a regluarized (l1) logistic regression\n",
    "3. Evaluate it using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V257' 'V246' 'V244' 'V242' 'V45' 'V201' 'V200' 'V86' 'V87' 'V189' 'V44'\n",
      " 'V188' 'V258' 'V52' 'V51' 'V228' 'V170' 'V40' 'V79' 'V39']\n"
     ]
    }
   ],
   "source": [
    "strong_vars = corrs.index.values[:20]\n",
    "print(strong_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subset = x_trans[strong_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fraud_logit = LogisticRegression(penalty='l1')\n",
    "\n",
    "fraud_logit_fit = fraud_logit.fit(X=x_subset,y=fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "0       0.025443\n",
      "1       0.021101\n",
      "2       0.021101\n",
      "3       0.021101\n",
      "4       0.024571\n",
      "5       0.021101\n",
      "6       0.021101\n",
      "7       0.021101\n",
      "8       0.024571\n",
      "9       0.021101\n",
      "10      0.025903\n",
      "11      0.059644\n",
      "12      0.025118\n",
      "13      0.021101\n",
      "14      0.021101\n",
      "15      0.025443\n",
      "16      0.024571\n",
      "17      0.024571\n",
      "18      0.021101\n",
      "19      0.021101\n",
      "20      0.021101\n",
      "21      0.021101\n",
      "22      0.025761\n",
      "23      0.021101\n",
      "24      0.021101\n",
      "25      0.061239\n",
      "26      0.021101\n",
      "27      0.021101\n",
      "28      0.025443\n",
      "29      0.021101\n",
      "...          ...\n",
      "590510  0.021101\n",
      "590511  0.021101\n",
      "590512  0.021101\n",
      "590513  0.021101\n",
      "590514  0.021101\n",
      "590515  0.104315\n",
      "590516  0.021101\n",
      "590517  0.021101\n",
      "590518  0.021101\n",
      "590519  0.021101\n",
      "590520  0.021101\n",
      "590521  0.001546\n",
      "590522  0.021101\n",
      "590523  0.021101\n",
      "590524  0.021101\n",
      "590525  0.021101\n",
      "590526  0.025761\n",
      "590527  0.034847\n",
      "590528  0.035845\n",
      "590529  0.059644\n",
      "590530  0.021101\n",
      "590531  0.025761\n",
      "590532  0.021101\n",
      "590533  0.021101\n",
      "590534  0.062902\n",
      "590535  0.025443\n",
      "590536  0.021101\n",
      "590537  0.021101\n",
      "590538  0.046503\n",
      "590539  0.035845\n",
      "\n",
      "[590540 rows x 1 columns]\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "5         0\n",
      "6         0\n",
      "7         0\n",
      "8         0\n",
      "9         0\n",
      "10        0\n",
      "11        0\n",
      "12        0\n",
      "13        0\n",
      "14        0\n",
      "15        0\n",
      "16        0\n",
      "17        0\n",
      "18        0\n",
      "19        0\n",
      "20        0\n",
      "21        0\n",
      "22        0\n",
      "23        0\n",
      "24        0\n",
      "25        0\n",
      "26        0\n",
      "27        0\n",
      "28        0\n",
      "29        0\n",
      "         ..\n",
      "590510    0\n",
      "590511    0\n",
      "590512    0\n",
      "590513    0\n",
      "590514    0\n",
      "590515    0\n",
      "590516    0\n",
      "590517    0\n",
      "590518    0\n",
      "590519    0\n",
      "590520    0\n",
      "590521    0\n",
      "590522    0\n",
      "590523    0\n",
      "590524    0\n",
      "590525    0\n",
      "590526    1\n",
      "590527    0\n",
      "590528    0\n",
      "590529    0\n",
      "590530    0\n",
      "590531    0\n",
      "590532    0\n",
      "590533    0\n",
      "590534    0\n",
      "590535    0\n",
      "590536    0\n",
      "590537    0\n",
      "590538    0\n",
      "590539    0\n",
      "Name: isFraud, Length: 590540, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fitted = fraud_logit_fit.predict_proba(X=x_subset)\n",
    "fraud_prob = pd.DataFrame(fitted[:,1])\n",
    "print(fraud_prob)\n",
    "print(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066583427623736\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(fraud,fraud_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial estimates of the AUC score, not that great considering the Kaggle leaderboard have scores of 0.94+. This is probably higher than what the actual score will be given that I didn't do any cross-validation. Now I'm going to evaluate it using cross-validation to get a proper feel for our out of sample AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_preds = ms.cross_val_predict(fraud_logit, X=x_subset, y=fraud, cv=3,method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7072765140773767\n"
     ]
    }
   ],
   "source": [
    "cv_score = roc_auc_score(fraud,cv_preds[:,1])\n",
    "print(cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, seems like it generalises reasonably well. Note that using cross_val_predict isn't the recommended way to evaluate generalisation error (at least according to the documentation), I'm doing it this way because cross_val_score does not allow me to use predict_proba as the predicted values but uses the predict() method and returns either 0 or 1. This is not desirable as the competition metric is the roc_auc_score based on the predicted probabilities, not the \\[0,1\\] classification.\n",
    "\n",
    "Next, we'll try playing around with the number of variables included in the explanatory variable set. I cut it down to 20 variables based on their absolute correlation with the dependent variable. Let's see how increasing/decreasing it has an effect on the cross-validated ROC/AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "var_count = np.arange(20,30,2)\n",
    "cv_scores = []\n",
    "for var in var_count:\n",
    "    strong_vars = corrs.index.values[:var]\n",
    "    x_subset = x_trans[strong_vars]\n",
    "    cv_preds = ms.cross_val_predict(fraud_logit, X=x_subset, y=fraud, cv=3,method='predict_proba')\n",
    "    cv_score = roc_auc_score(fraud,cv_preds[:,1])\n",
    "    cv_scores.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7073550353571013, 0.7113295167667276, 0.7126862469248098, 0.7125710671743295, 0.7108850213335886]\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much difference in the AUC scores, seems like the optimum is around 20-30 variables. \n",
    "\n",
    "We'll also play around with the regularisation parameter, however I don't think that there will be much potential for improvement but we'll see how we go. The logistic regression has a few options for regularisation, l1, l2 and elastic net (a combination of l1 and l2 I think). We'll try the three different options and different values for C, the inverse of regularisation strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            V257      V246      V244      V242       V45      V201      V200  \\\n",
      "0       1.250993  1.183723  1.118562  1.113463  1.120779  1.159106  1.119977   \n",
      "1       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "2       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "3       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "4       1.000000  1.000000  1.000000  1.000000  1.120779  1.000000  1.000000   \n",
      "5       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "6       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "7       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "8       1.000000  1.000000  1.000000  1.000000  1.120779  1.000000  1.000000   \n",
      "9       1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "10      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "11      1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "12      1.250993  1.183723  1.118562  1.113463  2.000000  1.159106  1.119977   \n",
      "13      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "14      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "15      1.250993  1.183723  1.118562  1.113463  1.120779  1.159106  1.119977   \n",
      "16      1.000000  1.000000  1.000000  1.000000  1.120779  1.000000  1.000000   \n",
      "17      1.000000  1.000000  1.000000  1.000000  1.120779  1.000000  1.000000   \n",
      "18      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "19      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "20      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "21      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "22      1.000000  1.000000  1.000000  1.000000  1.120779  1.159106  1.119977   \n",
      "23      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "24      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "25      1.250993  1.183723  1.118562  1.113463  2.000000  1.159106  1.119977   \n",
      "26      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "27      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "28      1.250993  1.183723  1.118562  1.113463  1.120779  1.159106  1.119977   \n",
      "29      1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "590510  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590511  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590512  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590513  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590514  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590515  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590516  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590517  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590518  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590519  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590520  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590521  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "590522  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590523  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590524  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590525  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590526  1.000000  1.000000  1.000000  1.000000  1.120779  1.159106  1.119977   \n",
      "590527  1.250993  1.183723  1.118562  1.113463  2.000000  1.159106  1.119977   \n",
      "590528  1.250993  1.183723  1.118562  1.113463  2.000000  1.159106  1.119977   \n",
      "590529  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "590530  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590531  1.000000  1.000000  1.000000  1.000000  1.120779  1.159106  1.119977   \n",
      "590532  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590533  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590534  1.250993  1.183723  1.118562  1.113463  1.000000  1.000000  1.000000   \n",
      "590535  1.250993  1.183723  1.118562  1.113463  1.120779  1.159106  1.119977   \n",
      "590536  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590537  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590538  1.250993  1.183723  1.118562  1.113463  1.000000  1.159106  1.119977   \n",
      "590539  1.250993  1.183723  1.118562  1.113463  2.000000  1.159106  1.119977   \n",
      "\n",
      "             V86       V87      V189  ...       V79       V39       V94  \\\n",
      "0       1.000000  1.000000  1.038314  ...  0.000000  0.166076  0.000000   \n",
      "1       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "2       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "3       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "4       1.064885  1.099456  1.000000  ...  0.136867  0.166076  0.137007   \n",
      "5       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "6       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "7       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "8       1.064885  1.099456  1.000000  ...  0.136867  0.166076  0.137007   \n",
      "9       1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "10      1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
      "11      1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
      "12      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "13      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "14      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "15      1.000000  1.000000  1.038314  ...  0.000000  0.166076  0.000000   \n",
      "16      1.064885  1.099456  1.000000  ...  0.136867  0.166076  0.137007   \n",
      "17      1.064885  1.099456  1.000000  ...  0.136867  0.166076  0.137007   \n",
      "18      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "19      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "20      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "21      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "22      1.064885  1.099456  1.038314  ...  0.136867  0.166076  0.137007   \n",
      "23      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "24      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "25      1.000000  3.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "26      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "27      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "28      1.000000  1.000000  1.038314  ...  0.000000  0.166076  0.000000   \n",
      "29      1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "590510  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590511  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590512  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590513  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590514  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590515  1.000000  6.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590516  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590517  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590518  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590519  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590520  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590521  1.000000  1.000000  1.000000  ...  1.000000  2.000000  1.000000   \n",
      "590522  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590523  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590524  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590525  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590526  1.064885  1.099456  1.038314  ...  0.136867  0.166076  0.137007   \n",
      "590527  1.000000  2.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590528  2.000000  2.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590529  1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
      "590530  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590531  1.064885  1.099456  1.038314  ...  0.136867  0.166076  0.137007   \n",
      "590532  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590533  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590534  1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
      "590535  1.000000  1.000000  1.038314  ...  0.000000  0.166076  0.000000   \n",
      "590536  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590537  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590538  1.000000  1.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "590539  2.000000  2.000000  1.038314  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "           V38       V43       V33      V199      V17       V18       V74  \n",
      "0       1.1624  0.168942  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "1       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "2       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "3       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "4       1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "5       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "6       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "7       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "8       1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "9       1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "10      4.0000  1.000000  1.000000  1.000000  1.00000  1.000000  2.000000  \n",
      "11      1.0000  1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
      "12      2.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "13      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "14      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "15      1.1624  0.168942  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "16      1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "17      1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "18      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "19      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "20      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "21      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "22      1.1624  0.168942  0.130693  1.270749  0.13404  0.135363  0.152147  \n",
      "23      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "24      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "25      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "26      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "27      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "28      1.1624  0.168942  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "29      1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "...        ...       ...       ...       ...      ...       ...       ...  \n",
      "590510  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590511  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590512  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590513  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590514  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590515  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590516  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590517  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590518  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590519  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590520  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590521  9.0000  2.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
      "590522  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590523  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590524  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590525  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590526  1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "590527  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590528  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590529  1.0000  1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
      "590530  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590531  1.1624  0.168942  0.130693  1.000000  0.13404  0.135363  0.152147  \n",
      "590532  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590533  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590534  1.0000  1.000000  1.000000  1.000000  1.00000  1.000000  1.000000  \n",
      "590535  1.1624  0.168942  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590536  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590537  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "590538  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  1.000000  \n",
      "590539  1.0000  0.000000  0.000000  1.270749  0.00000  0.000000  0.000000  \n",
      "\n",
      "[590540 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reg_params = [10, 5, 2, 1, 0.5, 0.1, 0.05, 0.01, 0.001, 0.0001]\n",
    "\n",
    "cv_scores = []\n",
    "for c in reg_params:\n",
    "    strong_vars = corrs.index.values[:25]\n",
    "    x_subset = x_trans[strong_vars]\n",
    "    fraud_logit = LogisticRegression(penalty='l1',solver='saga', C=c)\n",
    "    cv_preds = ms.cross_val_predict(fraud_logit, X=x_subset, y=fraud, cv=3,method='predict_proba')\n",
    "    cv_score = roc_auc_score(fraud,cv_preds[:,1])\n",
    "    cv_scores.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7149914231163619, 0.7150168693689567, 0.7150039055283233, 0.715050968514276, 0.715104815576696, 0.7143748083980866, 0.7145054272832961, 0.7129334684883738, 0.7116303422156077, 0.5909718872880813]\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all honesty, very little difference at all. We'll try using l2 and elastic net regularisation as well, but I think this is a dead end. We'll try some other algorithms such as random forests, kNN support and boosting methods (XGB/LGB/GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "reg_params = [10, 5, 2, 1, 0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "\n",
    "cv_scores = []\n",
    "for c in reg_params:\n",
    "    strong_vars = corrs.index.values[:25]\n",
    "    x_subset = x_trans[strong_vars]\n",
    "    fraud_logit = LogisticRegression(penalty='l2',solver='saga', C=c)\n",
    "    cv_preds = ms.cross_val_predict(fraud_logit, X=x_subset, y=fraud, cv=3,method='predict_proba')\n",
    "    cv_score = roc_auc_score(fraud,cv_preds[:,1])\n",
    "    cv_scores.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7147704652745053, 0.7148037511968637, 0.7147992965591834, 0.71479571662874, 0.7148091787128064, 0.7148363747195667, 0.7139170667552304, 0.7137825881606463, 0.7115933563665607]\n"
     ]
    }
   ],
   "source": [
    "print(cv_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
